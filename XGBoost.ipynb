{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWPgdChEDItf"
   },
   "source": [
    "# Performance Objective:\n",
    "The fraud incidence rate in this dataset is ~1%. For rare-event detection, accuracy becomes a far less meaningful metric than precision and recall. For bank account fraud in particular, missing a fraud instance is far more financially costly than falsely flagging a non-fraud instance. Therefore, we weigh recall significantly more than precision. Given this, we chose the F2 metric as the optimization goal for our models, reflecting real-world business objectives.\n",
    "\n",
    "# Data Preprocessing:\n",
    "- Train/test split: First 6 months for training, last 2 months for testing\n",
    "- Drop the ‘month’ column.\n",
    "- Drop the 'device_fraud_count' column. It is a constant-valued column (useless for prediction)\n",
    "- One-hot encoding of categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNXotvPNSw3B"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Base.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccoHhBNJSzKq",
    "outputId": "048e1faa-69f3-4537-b38a-859d03be4652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training data # rows: 794989\n",
      "Full test data # rows: 205011\n",
      "Column list: Index(['fraud_bool', 'income', 'name_email_similarity',\n",
      "       'prev_address_months_count', 'current_address_months_count',\n",
      "       'customer_age', 'days_since_request', 'intended_balcon_amount',\n",
      "       'payment_type', 'zip_count_4w', 'velocity_6h', 'velocity_24h',\n",
      "       'velocity_4w', 'bank_branch_count_8w',\n",
      "       'date_of_birth_distinct_emails_4w', 'employment_status',\n",
      "       'credit_risk_score', 'email_is_free', 'housing_status',\n",
      "       'phone_home_valid', 'phone_mobile_valid', 'bank_months_count',\n",
      "       'has_other_cards', 'proposed_credit_limit', 'foreign_request', 'source',\n",
      "       'session_length_in_minutes', 'device_os', 'keep_alive_session',\n",
      "       'device_distinct_emails_8w'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# data processing\n",
    "mask = df[\"month\"] <= 5\n",
    "full_training_data = df[mask].sample(frac=1).reset_index(drop=True).drop('month',axis=1) # train on months 0 to 5. drop month as a feature\n",
    "full_test_data = df[~mask].sample(frac=1).reset_index(drop=True).drop('month',axis=1) # test on months 6 and 7. drop month as a feature\n",
    "\n",
    "# 'device_fraud_count' is literally a constant column. get rid of it.\n",
    "full_training_data = full_training_data.drop('device_fraud_count',axis=1)\n",
    "full_test_data = full_test_data.drop('device_fraud_count',axis=1)\n",
    "\n",
    "print(\"Full training data # rows: \" + str(full_training_data.shape[0]))\n",
    "print(\"Full test data # rows: \" + str(full_test_data.shape[0]))\n",
    "print(\"Column list: \" + str(full_training_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fG6YRoXbTDRm",
    "outputId": "906dca74-715a-4922-e210-12b720b7c98e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n"
     ]
    }
   ],
   "source": [
    "# more data processing\n",
    "y_train_full = full_training_data[\"fraud_bool\"]\n",
    "X_train_full = full_training_data.drop(\"fraud_bool\",axis=1)\n",
    "y_test_full = full_test_data[\"fraud_bool\"]\n",
    "X_test_full = full_test_data.drop(\"fraud_bool\",axis=1)\n",
    "\n",
    "# make sure all numerical columns are actually stored numerically\n",
    "y_train_full = y_train_full.astype(float)\n",
    "y_test_full = y_test_full.astype(float)\n",
    "for col in X_train_full.columns:\n",
    "    converted = pd.to_numeric(X_train_full[col], errors='coerce')\n",
    "    if converted.notna().sum() == X_train_full[col].notna().sum():\n",
    "        X_train_full[col] = converted\n",
    "for col in X_test_full.columns:\n",
    "    converted = pd.to_numeric(X_test_full[col], errors='coerce')\n",
    "    if converted.notna().sum() == X_test_full[col].notna().sum():\n",
    "        X_test_full[col] = converted\n",
    "\n",
    "categorical_cols = X_train_full.select_dtypes(exclude='number').columns.tolist()\n",
    "print(\"Categorical columns: \" + str(categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azG6RvTmU27K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Dealing with missing values\n",
    "# the following columns have \"-1\" to indicate missing values: prev_address_months_count, current_address_months_count, bank_months_count, session_length_in_minutes\n",
    "\n",
    "missing_cols = [\n",
    "    \"prev_address_months_count\",\n",
    "    \"current_address_months_count\",\n",
    "    \"bank_months_count\",\n",
    "    \"session_length_in_minutes\"\n",
    "]\n",
    "\n",
    "# Convert -1 to NaN in both train and test\n",
    "X_train_full[missing_cols] = X_train_full[missing_cols].replace(-1, np.nan)\n",
    "X_test_full[missing_cols] = X_test_full[missing_cols].replace(-1, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-HNJHTeU-24",
    "outputId": "0ebb75b7-3b75-4ee2-b10e-52474fd31976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(794989, 45)\n",
      "(205011, 45)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "ohe = OneHotEncoder(\n",
    "    drop=\"first\",\n",
    "    handle_unknown=\"ignore\",\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"cat\", ohe, categorical_cols)],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "preprocessor.set_output(transform=\"pandas\")\n",
    "\n",
    "X_train_full = preprocessor.fit_transform(X_train_full)\n",
    "X_test_full  = preprocessor.transform(X_test_full)\n",
    "\n",
    "# transform into numpy arrays\n",
    "X_train_full = X_train_full.to_numpy()\n",
    "X_test_full = X_test_full.to_numpy()\n",
    "\n",
    "print(X_train_full.shape)\n",
    "print(X_test_full.shape)\n",
    "\n",
    "print(type(X_train_full))\n",
    "print(type(X_test_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIZOO5szXquN"
   },
   "source": [
    "# Methodology:\n",
    "Our final model will have the following format:\n",
    "```\n",
    "import xgboost as xgb\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"max_depth\": max_depth,\n",
    "    \"min_child_weight\": min_child_weight,\n",
    "    \"subsample\": subsample,\n",
    "    \"colsample_bytree\": colsample_bytree,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"aucpr\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"verbosity\": 0,\n",
    "}\n",
    "\n",
    "model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=best_iteration_int,\n",
    ")\n",
    "\n",
    "y_test_prob = model.predict(dtest)\n",
    "y_test_pred = (y_test_prob >= best_threshold).astype(int)\n",
    "```\n",
    "We'll first tune the following hyperparameters using a stratified 5-fold CV, with F2 as the metric being optimized for:\n",
    "\n",
    "- learning_rate/eta (range: logspace from 0.03 to 0.2)\n",
    "- max_depth (range: uniform on 3:8)\n",
    "- min_child_weight (range: uniform on [1,10])\n",
    "- subsample (range: uniform on [0.7,1.0])\n",
    "- colsample_bytree (range: uniform on [0.6,1])\n",
    "- scale_pos_weight (range: logspace from [10,200])\n",
    "- best_threshold (range: linspace from [0,1])\n",
    "\n",
    "Note that best_iteration_int will be implicitly tuned for when we take the average of the early stopping iterations of the best hyperparameter combination across all splits.\n",
    "\n",
    "Because XGBoost's Python library implementations do not support F2 scores as a metric for early stopping, we will instead use PR AUC as the early stopping metric, which serves as a good surrogate for F2 because it too rewards good overall precision and recall.\n",
    "\n",
    "Due to the curse of dimensionality making the hyperparameter search grid combinatorially infeasible to cover via a systematic/exhaustive search, we will use random search instead. We will randomly sample 20 different (learning_rate, max_depth, min_child_weight, subsample, colsample_bytree, scale_pos_weight) hyperparameter combinations from their corresponding ranges, then concatenate each of these combinations with threshold values from the linspace of [0,1], producing a list of (learning_rate, max_depth, min_child_weight, subsample, colsample_bytree, scale_pos_weight, threshold) hyperparameter candidates.\n",
    "\n",
    "Our final model will then be trained on the full training data with the hyperparameters that we have found from the CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtZGZBKwb9ml",
    "outputId": "9ed90d43-3281-4c5c-c4f6-1006dad97152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.04370752583739313, 5, 8.790614226800376, 0.9923663120961845, 0.8436168227869654, 18.230416205212666), (0.03981751334779519, 3, 2.9573939461146663, 0.7885565412597443, 0.6145693355755574, 101.13212968988198), (0.1104677673822609, 6, 5.903898250808235, 0.7103998703434149, 0.836721774162202, 29.67350531839289), (0.14997910131165385, 6, 8.482588582187637, 0.73307060844429, 0.6216427912740435, 108.8590375047525), (0.0648764138658463, 5, 6.142196878076124, 0.7624978333041824, 0.7734773068432919, 56.59179326118033), (0.1405467118239003, 5, 5.913580631784271, 0.9444077551534324, 0.7005513034157849, 119.03127811045933), (0.16877813037694922, 4, 8.925293229164936, 0.7899119564484125, 0.7550660115789275, 73.55630382239742), (0.1769910266779769, 7, 6.193380236997042, 0.7279214279537065, 0.9345642246845189, 10.667846811683878), (0.050247437161997976, 5, 9.025677620610027, 0.977511594016963, 0.8633758674842724, 74.84121479891142), (0.03841768852534073, 4, 7.411261235149855, 0.8108707388774795, 0.9518822262265176, 65.25645752854594), (0.06171184631285087, 3, 9.400860129136118, 0.7741235778036006, 0.7017163522086355, 10.92551522008694), (0.13749318373384953, 5, 6.674059009764301, 0.934161192864195, 0.6457516459011837, 52.75385628367859), (0.07558067517620218, 3, 6.996826504194211, 0.7269896887874995, 0.837891862366142, 115.64591512482473), (0.1527232980936231, 6, 3.843705862061592, 0.7600934007987252, 0.8105333418561658, 49.43334474800835), (0.05536211413020388, 5, 4.697629220160701, 0.9775534762500341, 0.6785524701848478, 144.28223362604712), (0.09478350132948687, 3, 5.957399500936749, 0.7887860159663301, 0.6038218579044698, 26.659773433566215), (0.030874818950395483, 8, 7.693755940772998, 0.7244758813265239, 0.6911815800369256, 10.712943639199052), (0.17709746490701544, 4, 9.34313717634192, 0.7735007132590859, 0.9295453364649093, 81.74514746808127), (0.05369190763236915, 7, 2.1338330931897245, 0.7652865804635937, 0.8711449248569689, 19.05441495557614), (0.13303630732928357, 7, 8.63106457848684, 0.9003196330154198, 0.6642030688346802, 10.347828427289278)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def sample_hyperparams(n_samples=20):\n",
    "\n",
    "    combos = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "\n",
    "        # learning_rate: logspace from 0.03 to 0.2 (base 10)\n",
    "        lr_exp = np.random.uniform(np.log10(0.03), np.log10(0.2))\n",
    "        learning_rate = 10 ** lr_exp\n",
    "\n",
    "        # max_depth: integer uniform in [3,8]\n",
    "        max_depth = np.random.randint(3, 9)\n",
    "\n",
    "        # min_child_weight: uniform in [1,10]\n",
    "        min_child_weight = np.random.uniform(1, 10)\n",
    "\n",
    "        # subsample: uniform in [0.7,1.0]\n",
    "        subsample = np.random.uniform(0.7, 1.0)\n",
    "\n",
    "        # colsample_bytree: uniform in [0.6,1.0]\n",
    "        colsample_bytree = np.random.uniform(0.6, 1.0)\n",
    "\n",
    "        # scale_pos_weight: logspace from 10 to 200 (base 10)\n",
    "        spw_exp = np.random.uniform(np.log10(10), np.log10(200))\n",
    "        scale_pos_weight = 10 ** spw_exp\n",
    "\n",
    "        combos.append((learning_rate, max_depth, min_child_weight, subsample, colsample_bytree, scale_pos_weight))\n",
    "\n",
    "    return combos\n",
    "\n",
    "n_hypers = 20\n",
    "hyperparameter_combinations = sample_hyperparams(n_hypers)\n",
    "print(hyperparameter_combinations)\n",
    "\n",
    "# threshold candidates: 25 values from 0 to 1\n",
    "thresholds = np.linspace(0.0, 1.0, 25)\n",
    "\n",
    "full_hyperparameter_combinations = [hyper + (threshold,) for hyper in hyperparameter_combinations for threshold in thresholds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_XU0JQ76-tV",
    "outputId": "f9d36839-782c-44ea-f896-e128721fb2d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "models fitted: 100%|████████████████████████████████████████████| 100/100 [1:18:47<00:00, 47.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: (0.06171184631285087, 3, 9.400860129136118, 0.7741235778036006, 0.7017163522086355, 10.92551522008694, np.float64(0.375))\n",
      "Best iteration: 540.2\n",
      "Mean F2 across folds: 0.3076254782687504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "running_F2_scores = dict.fromkeys(full_hyperparameter_combinations, 0.0) # stores running total of F2 scores across folds to average later\n",
    "running_best_iteration = dict.fromkeys(full_hyperparameter_combinations, 0.0) # stores running total of best iteration across folds to average later\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state = 23)\n",
    "total_trains = n_splits * n_hypers\n",
    "progress_bar = tqdm(total=total_trains, desc=\"models fitted\", ncols=100)\n",
    "\n",
    "for train_ids, val_ids in skf.split(X_train_full, y_train_full):\n",
    "    X_train_inner, X_val_inner = X_train_full[train_ids], X_train_full[val_ids]\n",
    "    y_train_inner, y_val_inner = y_train_full[train_ids], y_train_full[val_ids]\n",
    "    dtrain = xgb.DMatrix(X_train_inner, label=y_train_inner)\n",
    "    dval = xgb.DMatrix(X_val_inner, label=y_val_inner)\n",
    "\n",
    "    for hyper in hyperparameter_combinations:\n",
    "      learning_rate, max_depth, min_child_weight, subsample, colsample_bytree, scale_pos_weight = hyper\n",
    "\n",
    "      params = {\n",
    "          \"learning_rate\": learning_rate,\n",
    "          \"max_depth\": max_depth,\n",
    "          \"min_child_weight\": min_child_weight,\n",
    "          \"subsample\": subsample,\n",
    "          \"colsample_bytree\": colsample_bytree,\n",
    "          \"scale_pos_weight\": scale_pos_weight,\n",
    "          \"objective\": \"binary:logistic\",\n",
    "          \"eval_metric\": \"aucpr\",\n",
    "          \"tree_method\": \"hist\",\n",
    "      }\n",
    "      model = xgb.train(\n",
    "          params,\n",
    "          dtrain,\n",
    "          num_boost_round=1000,\n",
    "          evals=[(dval, \"val\")],\n",
    "          early_stopping_rounds=50,\n",
    "          verbose_eval=False\n",
    "      )\n",
    "      progress_bar.update(1)\n",
    "\n",
    "      best_iteration = model.best_iteration\n",
    "\n",
    "      y_val_prob = model.predict(dval) # P(y=1)\n",
    "\n",
    "      # Evaluate F2 for each threshold\n",
    "      for t in thresholds:\n",
    "          y_val_pred = (y_val_prob >= t).astype(int)\n",
    "          f2 = fbeta_score(y_val_inner, y_val_pred, beta=2, zero_division=0)\n",
    "          running_F2_scores[hyper + (t,)] += f2\n",
    "          running_best_iteration[hyper + (t,)] += best_iteration\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "# Average over folds\n",
    "for full_hyper in full_hyperparameter_combinations:\n",
    "    running_F2_scores[full_hyper] /= n_splits\n",
    "    running_best_iteration[full_hyper] /= n_splits\n",
    "\n",
    "# Select the best hyperparameter set by F2 score\n",
    "best_hyper = max(running_F2_scores, key=running_F2_scores.get)\n",
    "best_iteration = running_best_iteration[best_hyper]\n",
    "best_f2 = running_F2_scores[best_hyper]\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hyper)\n",
    "print(\"Best iteration:\", best_iteration)\n",
    "print(\"Mean F2 across folds:\", best_f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQU5osuU_t35"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# train the final model\n",
    "\n",
    "learning_rate, max_depth, min_child_weight, subsample, colsample_bytree, scale_pos_weight, best_threshold = (0.06171184631285087, 3, 9.400860129136118, 0.7741235778036006, 0.7017163522086355, 10.92551522008694, 0.375)\n",
    "best_iteration_int = 540\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_full, label=y_train_full)\n",
    "dtest = xgb.DMatrix(X_test_full)\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"max_depth\": max_depth,\n",
    "    \"min_child_weight\": min_child_weight,\n",
    "    \"subsample\": subsample,\n",
    "    \"colsample_bytree\": colsample_bytree,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"aucpr\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"verbosity\": 0,\n",
    "}\n",
    "\n",
    "model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=best_iteration_int,\n",
    ")\n",
    "\n",
    "y_test_prob = model.predict(dtest)\n",
    "y_test_pred = (y_test_prob >= best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZypeypSN_xo4",
    "outputId": "55c2e098-f0ee-4c79-b42e-b07be54e3098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model F2 Score: 0.34628858170800986\n",
      "Final model Recall: 0.5083391243919388\n",
      "Final model Precision: 0.15220557636287974\n",
      "Final model Accuracy: 0.9533488446961382\n",
      "Final model PR AUC: 0.20025156289995072\n",
      "Final model ROC AUC: 0.895888985388483\n"
     ]
    }
   ],
   "source": [
    "# F2 Score:\n",
    "f2 = fbeta_score(y_test_full, y_test_pred, beta=2, zero_division=0)\n",
    "print(\"Final model F2 Score: \" + str(f2))\n",
    "\n",
    "# Recall:\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_test_full, y_test_pred)\n",
    "print(\"Final model Recall: \" + str(recall))\n",
    "\n",
    "# Precision:\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test_full, y_test_pred)\n",
    "print(\"Final model Precision: \" + str(precision))\n",
    "\n",
    "# Accuracy:\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test_full, y_test_pred)\n",
    "print(\"Final model Accuracy: \" + str(accuracy))\n",
    "\n",
    "# PR AUC:\n",
    "from sklearn.metrics import average_precision_score\n",
    "pr_auc = average_precision_score(y_test_full, y_test_prob)\n",
    "print(\"Final model PR AUC: \" + str(pr_auc))\n",
    "\n",
    "# ROC AUC:\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc = roc_auc_score(y_test_full, y_test_prob)\n",
    "print(\"Final model ROC AUC: \" + str(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QrY-xde262v"
   },
   "source": [
    "Our XGBoost method achieved the following results on the test data:\n",
    "```\n",
    "Final model F2 Score: 0.34628858170800986\n",
    "Final model Recall: 0.5083391243919388\n",
    "Final model Precision: 0.15220557636287974\n",
    "Final model Accuracy: 0.9533488446961382\n",
    "```\n",
    "Compare this to our RBF kernel SVM ensemble method's results:\n",
    "```\n",
    "Final model F2 Score: 0.31284385315594676\n",
    "Final model Recall: 0.4346768589298124\n",
    "Final model Precision: 0.14748879981136526\n",
    "Final model Accuracy: 0.956792562350313\n",
    "```\n",
    "Accuracy and precision remain roughly the same, but we see a significant relative performance gain in recall of roughly (0.5083391243919388/0.4346768589298124) - 1 = ~17% and a meaningful relative performance gain in F2 score of roughly (0.34628858170800986/0.31284385315594676) - 1 = ~11%.\n",
    "\n",
    "## Comparison to public baselines:\n",
    "We compare our model to strong baseline models provided in the most popular publicly available notebook for this dataset: https://www.kaggle.com/code/lennart4711/baselinemodels-roc. In their code, they used industry-standard models with balanced class weights to modify their objective functions to account for heavy class imbalance (~1% fraud rate into account), with zero hyperparameter searching. Note that their models are directly comparable to ours since they used an identical dataset (Base.csv) and an identical train/test split.\n",
    "\n",
    "After modifying their code to compute F2 scores for each model, I found that they achieved:\n",
    "- Logistic Regression: 0.3102\n",
    "- XGBoost: 0.2925\n",
    "- Random Forest: 0.2475\n",
    "- Neural Network: 0.3204\n",
    "\n",
    "Observe that our XGBoost model achieves meaningful relative performance gains of (0.34628858170800986/0.3204) - 1 = \\~8% over their best baseline model and (0.34628858170800986/0.2925) - 1 = \\~18% over their XGBoost model. Given that these baselines already use strong, well-established models which account for the heavy class imbalance (~1% fraud rate), and are therefore representative of realistic industry prototypes, this comparison reveals that our modeling choices offer meaningful, nontrivial performance improvements on this task.\n",
    "\n",
    "## Reflection and next steps:\n",
    "The fact remains, however, that XGBoost, linear kernel SVM, and RBF kernel SVM ensemble all achieved similar F2 scores in absolute terms (in the 0.31-0.35 range). Multiple distinct modelling techniques all achieving scores in this narrow range suggests that there is limited predictive signal left to exploit in our currently tabulated features. In other words, we are likely near the Pareto frontier with respect to our currently tabulated features.\n",
    "\n",
    "Significant improvements in our model performance will likely not be from the choice of modeling method, but from further feature engineering (i.e. deriving new features, exploring interactions, etc.). Therefore, as a next step for improving performance, the primary exploratory focus should be on feature engineering.\n",
    "\n",
    "Additionally, one may consider trying undersampling/oversampling techniques to boost the proportion of fraud data in the training set to give fraud data more \"attention\" during training. In our SVM and XGBoost codes, we primarily relied on re-weighting the objective function to heavily penalize fraud misclassification."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY-jcBfOQyLR"
   },
   "source": [
    "# Performance Objective:\n",
    "The fraud incidence rate in this dataset is ~1%. For rare-event detection, accuracy becomes a far less meaningful metric than precision and recall. For bank account fraud in particular, missing a fraud instance is far more financially costly than falsely flagging a non-fraud instance. Therefore, we weigh recall significantly more than precision. Given this, we chose the F2 metric as the optimization goal for our models, reflecting real-world business objectives.\n",
    "\n",
    "# Data preprocessing:\n",
    "- Train/test split: First 6 months for training, last 2 months for testing\n",
    "- Drop the ‘month’ column.\n",
    "- Drop the 'device_fraud_count' column. It is a constant-valued column (useless for prediction)\n",
    "- One-hot encoding of categorical columns\n",
    "- Create indicator columns for columns with missing values\n",
    "- Standardize columns\n",
    "- Mean imputation of missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evCpa42CYHOZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Base.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TkXm-ciXYcxV",
    "outputId": "6debcf2c-27ac-4982-e5c5-6cb6caf905fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training data # rows: 794989\n",
      "Full test data # rows: 205011\n",
      "Column list: Index(['fraud_bool', 'income', 'name_email_similarity',\n",
      "       'prev_address_months_count', 'current_address_months_count',\n",
      "       'customer_age', 'days_since_request', 'intended_balcon_amount',\n",
      "       'payment_type', 'zip_count_4w', 'velocity_6h', 'velocity_24h',\n",
      "       'velocity_4w', 'bank_branch_count_8w',\n",
      "       'date_of_birth_distinct_emails_4w', 'employment_status',\n",
      "       'credit_risk_score', 'email_is_free', 'housing_status',\n",
      "       'phone_home_valid', 'phone_mobile_valid', 'bank_months_count',\n",
      "       'has_other_cards', 'proposed_credit_limit', 'foreign_request', 'source',\n",
      "       'session_length_in_minutes', 'device_os', 'keep_alive_session',\n",
      "       'device_distinct_emails_8w'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# data processing\n",
    "mask = df[\"month\"] <= 5\n",
    "full_training_data = df[mask].sample(frac=1).reset_index(drop=True).drop('month',axis=1) # train on months 0 to 5. drop month as a feature\n",
    "full_test_data = df[~mask].sample(frac=1).reset_index(drop=True).drop('month',axis=1) # test on months 6 and 7. drop month as a feature\n",
    "\n",
    "# 'device_fraud_count' is literally a constant column. get rid of it.\n",
    "full_training_data = full_training_data.drop('device_fraud_count',axis=1)\n",
    "full_test_data = full_test_data.drop('device_fraud_count',axis=1)\n",
    "\n",
    "print(\"Full training data # rows: \" + str(full_training_data.shape[0]))\n",
    "print(\"Full test data # rows: \" + str(full_test_data.shape[0]))\n",
    "print(\"Column list: \" + str(full_training_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Xxx6-MceVvp",
    "outputId": "e6cfc109-c542-42d6-c70a-ad04744020b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of 1's in y_train_full: 0.010252972053701372\n",
      "Categorical columns: ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n"
     ]
    }
   ],
   "source": [
    "# more data processing\n",
    "y_train_full = full_training_data[\"fraud_bool\"]\n",
    "X_train_full = full_training_data.drop(\"fraud_bool\",axis=1)\n",
    "y_test_full = full_test_data[\"fraud_bool\"]\n",
    "X_test_full = full_test_data.drop(\"fraud_bool\",axis=1)\n",
    "\n",
    "# percentage of 1's in y_train_full:\n",
    "print(\"Fraction of 1's in y_train_full: \" + str(y_train_full.mean()))\n",
    "\n",
    "# make sure all numerical columns are actually stored numerically\n",
    "y_train_full = y_train_full.astype(float)\n",
    "y_test_full = y_test_full.astype(float)\n",
    "for col in X_train_full.columns:\n",
    "    converted = pd.to_numeric(X_train_full[col], errors='coerce')\n",
    "    if converted.notna().sum() == X_train_full[col].notna().sum():\n",
    "        X_train_full[col] = converted\n",
    "for col in X_test_full.columns:\n",
    "    converted = pd.to_numeric(X_test_full[col], errors='coerce')\n",
    "    if converted.notna().sum() == X_test_full[col].notna().sum():\n",
    "        X_test_full[col] = converted\n",
    "\n",
    "categorical_cols = X_train_full.select_dtypes(exclude='number').columns.tolist()\n",
    "print(\"Categorical columns: \" + str(categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcZ2Bdcx5FE6"
   },
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "ohe = OneHotEncoder(\n",
    "    drop=\"first\",\n",
    "    handle_unknown=\"ignore\",\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"cat\", ohe, categorical_cols)],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "preprocessor.set_output(transform=\"pandas\")\n",
    "\n",
    "X_train_full = preprocessor.fit_transform(X_train_full)\n",
    "X_test_full  = preprocessor.transform(X_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yT-4HCFgZVD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Dealing with missing values\n",
    "# the following columns have \"-1\" to indicate missing values: prev_address_months_count, current_address_months_count, bank_months_count, session_length_in_minutes\n",
    "\n",
    "missing_cols = [\n",
    "    \"remainder__prev_address_months_count\",\n",
    "    \"remainder__current_address_months_count\",\n",
    "    \"remainder__bank_months_count\",\n",
    "    \"remainder__session_length_in_minutes\"\n",
    "]\n",
    "\n",
    "# 1. Convert -1 to NaN in both train and test\n",
    "X_train_full[missing_cols] = X_train_full[missing_cols].replace(-1, np.nan)\n",
    "X_test_full[missing_cols] = X_test_full[missing_cols].replace(-1, np.nan)\n",
    "\n",
    "# 1b. Create missing-value indicator columns (before imputation)\n",
    "for col in missing_cols:\n",
    "    indicator_name = f\"{col}_is_missing\"\n",
    "    X_train_full[indicator_name] = X_train_full[col].isna().astype(int)\n",
    "    X_test_full[indicator_name] = X_test_full[col].isna().astype(int)\n",
    "\n",
    "# create 2 numpy arrays storing the mean and sd of each column (not counting NaN values)\n",
    "col_means = X_train_full.mean().to_numpy()\n",
    "col_stds  = X_train_full.std().to_numpy()\n",
    "\n",
    "# 2. Mean imputer (fit only on training data)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "# Fit on training subset\n",
    "imputer.fit(X_train_full[missing_cols])\n",
    "\n",
    "# 3. Transform both train and test\n",
    "X_train_full[missing_cols] = imputer.transform(X_train_full[missing_cols])\n",
    "X_test_full[missing_cols] = imputer.transform(X_test_full[missing_cols])\n",
    "\n",
    "column_names = list(X_train_full.columns) # store column names for later\n",
    "\n",
    "# convert to numpy arrays\n",
    "X_train_full = X_train_full.to_numpy()\n",
    "X_test_full = X_test_full.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJLA6qKa8xgy",
    "outputId": "f497eafd-8f23-4ff7-a4bc-a9e6d9b5d3d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794989\n",
      "205011\n"
     ]
    }
   ],
   "source": [
    "# standardization of columns\n",
    "X_train_full = (X_train_full - col_means) / np.maximum(col_stds, 1e-8) # numerical stability\n",
    "X_test_full = (X_test_full - col_means) / np.maximum(col_stds, 1e-8)\n",
    "\n",
    "print(X_train_full.shape[0])\n",
    "print(X_test_full.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-6awJU2Jfuf",
    "outputId": "7c18d082-5ee2-4db4-c787-06807d5a2aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(794989, 49)\n",
      "(205011, 49)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_full.shape)\n",
    "print(X_test_full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJPmd2YdnGwo"
   },
   "source": [
    "# STAGE 1: LINEAR KERNEL SVM\n",
    "\n",
    "## Methodology:\n",
    "- The format of our model is LinearSVC(C=C,dual=False,class_weight={0: 1, 1: R}) (imported from sklearn.svm on Python).\n",
    "- Hyperparameters: C, R\n",
    "- First, we do a coarse-grained, stratified 3-fold logspace search for hyperparameters C and R in order to find the “magnitude neighborhoods” that the best hyperparameters live in. The evaluation metric used for this 3-fold search is F2. (C candidates: [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0], R candidates: [1.0, 10.0, 100.0, 1000.0])\n",
    "- After finding the best hyperparameter candidates from the coarse-grained search, we then do a fine-grained, stratified 3-fold linspace search for them in their neighborhoods. The evaluation metric used for this 3-fold search is again F2. This gives us our final hyperparameters.\n",
    "- We then train the final model on all of the training data using the C, R pair that we find from hyperparameter tuning.\n",
    "\n",
    "We first do a coarse-grained logspace search to find the magnitude(s) that the best margin loss penalty hyperparameters live in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jc3zh4vYnL9l",
    "outputId": "be8f0262-9231-4d0a-b200-740d84986b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
      "[1.0, 10.0, 100.0, 1000.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "# C hyperparameters\n",
    "C_values = np.logspace(-3, 3, 7).tolist() #1e-3 to 1e3\n",
    "print(C_values)\n",
    "\n",
    "# R weight hyperparameters:\n",
    "R_values = np.logspace(0,3,4).tolist()\n",
    "print(R_values)\n",
    "\n",
    "hyperparameter_combinations = list(product(C_values, R_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ISydrH-RyYeC",
    "outputId": "6821fb40-87ae-4877-f6ce-eed6d98a8fe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual inspection of coarse-grained hyperparameter performances: \n",
      "Average F2 score by hyperparameter combination: {(0.001, 1.0): 0.0, (0.001, 10.0): 0.1975937397846259, (0.001, 100.0): 0.16711202004468328, (0.001, 1000.0): 0.060256476857456394, (0.01, 1.0): 0.0, (0.01, 10.0): 0.19985552658267544, (0.01, 100.0): 0.1671428437968343, (0.01, 1000.0): 0.06025872031443463, (0.1, 1.0): 0.0, (0.1, 10.0): 0.20034146307836684, (0.1, 100.0): 0.16714542494311724, (0.1, 1000.0): 0.06025907831952048, (1.0, 1.0): 0.0, (1.0, 10.0): 0.20033640339053524, (1.0, 100.0): 0.16714798355157975, (1.0, 1000.0): 0.06025907831952048, (10.0, 1.0): 0.0, (10.0, 10.0): 0.20033163918169547, (10.0, 100.0): 0.16714714883221418, (10.0, 1000.0): 0.06025907831952048, (100.0, 1.0): 0.0, (100.0, 10.0): 0.20033163918169547, (100.0, 100.0): 0.16714714883221418, (100.0, 1000.0): 0.06025907831952048, (1000.0, 1.0): 0.0, (1000.0, 10.0): 0.20033163918169547, (1000.0, 100.0): 0.16714714883221418, (1000.0, 1000.0): 0.06025907831952048}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "running_F2_scores = dict.fromkeys(hyperparameter_combinations, 0) # stores running total of F2 scores across folds\n",
    "n_splits = 3\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state = 23)\n",
    "\n",
    "for train_ids, val_ids in skf.split(X_train_full, y_train_full):\n",
    "    X_train_inner, X_val_inner = X_train_full[train_ids], X_train_full[val_ids]\n",
    "    y_train_inner, y_val_inner = y_train_full[train_ids], y_train_full[val_ids]\n",
    "\n",
    "    for hyper in hyperparameter_combinations:\n",
    "        C = hyper[0]\n",
    "        R = hyper[1]\n",
    "        model = LinearSVC(C=C,dual=False,class_weight={0: 1, 1: R})\n",
    "        model.fit(X_train_inner, y_train_inner)\n",
    "        preds = model.predict(X_val_inner)\n",
    "\n",
    "        # F2 score\n",
    "        f2 = fbeta_score(y_val_inner, preds, beta=2, zero_division=0)\n",
    "        running_F2_scores[hyper] += f2\n",
    "\n",
    "average_F2_scores = {hyper: running_F2_scores[hyper] / n_splits for hyper in running_F2_scores}\n",
    "print(\"Visual inspection of coarse-grained hyperparameter performances: \")\n",
    "print(\"Average F2 score by hyperparameter combination: \" + str(average_F2_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb6NaiuY16mu",
    "outputId": "ee0c5a63-c542-4621-f414-f05685033885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1, 10.0) 0.20034146307836684\n",
      "(1.0, 10.0) 0.20033640339053524\n",
      "(10.0, 10.0) 0.20033163918169547\n",
      "(100.0, 10.0) 0.20033163918169547\n",
      "(1000.0, 10.0) 0.20033163918169547\n",
      "(0.01, 10.0) 0.19985552658267544\n",
      "(0.001, 10.0) 0.1975937397846259\n",
      "(1.0, 100.0) 0.16714798355157975\n",
      "(10.0, 100.0) 0.16714714883221418\n",
      "(100.0, 100.0) 0.16714714883221418\n",
      "(1000.0, 100.0) 0.16714714883221418\n",
      "(0.1, 100.0) 0.16714542494311724\n",
      "(0.01, 100.0) 0.1671428437968343\n",
      "(0.001, 100.0) 0.16711202004468328\n",
      "(0.1, 1000.0) 0.06025907831952048\n",
      "(1.0, 1000.0) 0.06025907831952048\n",
      "(10.0, 1000.0) 0.06025907831952048\n",
      "(100.0, 1000.0) 0.06025907831952048\n",
      "(1000.0, 1000.0) 0.06025907831952048\n",
      "(0.01, 1000.0) 0.06025872031443463\n",
      "(0.001, 1000.0) 0.060256476857456394\n",
      "(0.001, 1.0) 0.0\n",
      "(0.01, 1.0) 0.0\n",
      "(0.1, 1.0) 0.0\n",
      "(1.0, 1.0) 0.0\n",
      "(10.0, 1.0) 0.0\n",
      "(100.0, 1.0) 0.0\n",
      "(1000.0, 1.0) 0.0\n"
     ]
    }
   ],
   "source": [
    "# Sort the hyperparameters by F2 score:\n",
    "for hyper, score in sorted(average_F2_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(hyper, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FLeV0lQ3c0v"
   },
   "source": [
    "Notes:\n",
    "- C barely affects anything.\n",
    "- The optimal R seems to be in the range of [10,100]\n",
    "\n",
    "For our fine-grained hyperparameter search, we'll fix C = 1 and do a linear search of R from 10 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sT29hQxu4xMx",
    "outputId": "c9bd31d9-98cd-4665-de49-51b7c0495966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# R weight hyperparameters:\n",
    "R_values = np.linspace(10, 100, 10).tolist()\n",
    "print(R_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1gwsdDgJyvw",
    "outputId": "26f6e00a-aee1-48bd-c9db-969986bfdcef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best R: 20.0\n",
      "Best R's F2 score: 0.2795081464328278\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "running_F2_scores = dict.fromkeys(R_values, 0) # stores running total of F2 scores across folds\n",
    "n_splits = 3\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state = 23)\n",
    "\n",
    "for train_ids, val_ids in skf.split(X_train_full, y_train_full):\n",
    "    X_train_inner, X_val_inner = X_train_full[train_ids], X_train_full[val_ids]\n",
    "    y_train_inner, y_val_inner = y_train_full[train_ids], y_train_full[val_ids]\n",
    "\n",
    "    for R in R_values:\n",
    "        model = LinearSVC(C=1.0,dual=False,class_weight={0: 1, 1: R})\n",
    "        model.fit(X_train_inner, y_train_inner)\n",
    "        preds = model.predict(X_val_inner)\n",
    "\n",
    "        # F2 score\n",
    "        f2 = fbeta_score(y_val_inner, preds, beta=2, zero_division=0)\n",
    "        running_F2_scores[R] += f2\n",
    "\n",
    "average_F2_scores = {R: running_F2_scores[R] / n_splits for R in running_F2_scores}\n",
    "best_R = max(average_F2_scores, key=average_F2_scores.get)\n",
    "print(\"Best R: \" + str(best_R))\n",
    "print(\"Best R's F2 score: \" + str(average_F2_scores[best_R]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oqs-u8erP4id",
    "outputId": "503f5276-b07b-43c6-e834-998daccade19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model F2 Score: 0.31082471435533654\n",
      "Final model Recall: 0.3988881167477415\n",
      "Final model Precision: 0.1650611071171819\n",
      "Final model Accuracy: 0.963236119037515\n",
      "Final model ROC AUC: 0.8838175380071882\n",
      "Final model PR AUC: 0.1662443966266518\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import fbeta_score\n",
    "# Our final hyperparameters are C = 1.0, R = 20.0\n",
    "# train our final model\n",
    "final_model = LinearSVC(C=1.0,dual=False,class_weight={0: 1, 1: 20.0})\n",
    "final_model.fit(X_train_full, y_train_full)\n",
    "preds = final_model.predict(X_test_full)\n",
    "\n",
    "# F2 Score:\n",
    "f2 = fbeta_score(y_test_full, preds, beta=2, zero_division=0)\n",
    "print(\"Final model F2 Score: \" + str(f2))\n",
    "\n",
    "# Recall:\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_test_full, preds)\n",
    "print(\"Final model Recall: \" + str(recall))\n",
    "\n",
    "# Precision:\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test_full, preds)\n",
    "print(\"Final model Precision: \" + str(precision))\n",
    "\n",
    "# Accuracy:\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test_full, preds)\n",
    "print(\"Final model Accuracy: \" + str(accuracy))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Decision scores for AUCs\n",
    "scores = final_model.decision_function(X_test_full)\n",
    "\n",
    "# ROC AUC:\n",
    "roc_auc = roc_auc_score(y_test_full, scores)\n",
    "print(\"Final model ROC AUC:\", roc_auc)\n",
    "\n",
    "# PR AUC:\n",
    "pr_auc = average_precision_score(y_test_full, scores)\n",
    "print(\"Final model PR AUC:\", pr_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuUbfzTVlv8_",
    "outputId": "60967e2d-af74-4886-82c3-7f764ed805ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remainder__prev_address_months_count_is_missing 0.1840775408693057\n",
      "cat__device_os_windows 0.1726606370235882\n",
      "remainder__email_is_free 0.0902276679731933\n",
      "remainder__income 0.08539373697266404\n",
      "remainder__customer_age 0.07160413475476544\n",
      "cat__payment_type_AC 0.06656433909167216\n",
      "remainder__bank_months_count 0.06086480434083718\n",
      "remainder__credit_risk_score 0.05524611761741483\n",
      "remainder__device_distinct_emails_8w 0.05475204580850315\n",
      "cat__device_os_macintosh 0.05473006335153109\n",
      "remainder__proposed_credit_limit 0.041235656112572835\n",
      "remainder__prev_address_months_count 0.03794680122176087\n",
      "cat__device_os_other 0.03074820468014643\n",
      "remainder__zip_count_4w 0.02999213764554963\n",
      "remainder__foreign_request 0.0298911560350147\n",
      "remainder__velocity_4w 0.029156638983872867\n",
      "remainder__bank_months_count_is_missing 0.02593410624518672\n",
      "remainder__days_since_request 0.020579034998510815\n",
      "cat__payment_type_AB 0.018122089365314887\n",
      "cat__device_os_x11 0.016674212705907326\n",
      "cat__employment_status_CC 0.013096612492457013\n",
      "cat__payment_type_AD 0.013045964778562694\n",
      "remainder__velocity_24h 0.011617755551347673\n",
      "cat__source_TELEAPP 0.010121590405462049\n",
      "remainder__session_length_in_minutes 0.00909924089250809\n",
      "cat__employment_status_CG 0.0004051238315974678\n",
      "remainder__session_length_in_minutes_is_missing 0.00025955508282838094\n",
      "remainder__current_address_months_count_is_missing -0.0007507093223758436\n",
      "cat__payment_type_AE -0.0038534079263015104\n",
      "cat__housing_status_BG -0.007488056635474183\n",
      "remainder__velocity_6h -0.007538645164900795\n",
      "remainder__bank_branch_count_8w -0.010337174214137202\n",
      "remainder__current_address_months_count -0.014129775590706576\n",
      "cat__housing_status_BF -0.017031063384666457\n",
      "remainder__phone_mobile_valid -0.025585541143679863\n",
      "cat__employment_status_CD -0.027245842174148386\n",
      "remainder__intended_balcon_amount -0.028025382926783266\n",
      "cat__employment_status_CE -0.03414447718790753\n",
      "remainder__date_of_birth_distinct_emails_4w -0.03554843870308727\n",
      "cat__housing_status_BD -0.041013029409067696\n",
      "cat__employment_status_CF -0.050466701165121475\n",
      "cat__employment_status_CB -0.05975799157928456\n",
      "remainder__keep_alive_session -0.10241048431619434\n",
      "remainder__name_email_similarity -0.10825898209539894\n",
      "remainder__phone_home_valid -0.14472009825401563\n",
      "remainder__has_other_cards -0.14719090368602533\n",
      "cat__housing_status_BE -0.16965133844394345\n",
      "cat__housing_status_BC -0.17068864195033798\n",
      "cat__housing_status_BB -0.1738615613880953\n"
     ]
    }
   ],
   "source": [
    "# Get coefficient vector\n",
    "coef = final_model.coef_.ravel() # shape: (n_features,)\n",
    "\n",
    "# Pair each feature name with its coefficient\n",
    "feature_coefs = list(zip(column_names, coef))\n",
    "\n",
    "# Sort by coefficient in descending order\n",
    "feature_coefs_sorted = sorted(feature_coefs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for name, weight in feature_coefs_sorted:\n",
    "    print(name, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoNNc84RqFRP",
    "outputId": "b0b560b0-a93e-488f-ed17-515e61bbb1c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remainder__prev_address_months_count_is_missing 0.1840775408693057\n",
      "cat__device_os_windows 0.1726606370235882\n",
      "remainder__keep_alive_session -0.10241048431619434\n",
      "remainder__name_email_similarity -0.10825898209539894\n",
      "remainder__phone_home_valid -0.14472009825401563\n",
      "remainder__has_other_cards -0.14719090368602533\n",
      "cat__housing_status_BE -0.16965133844394345\n",
      "cat__housing_status_BC -0.17068864195033798\n",
      "cat__housing_status_BB -0.1738615613880953\n"
     ]
    }
   ],
   "source": [
    "# Find the most significant features\n",
    "# Filter the sorted coefficients\n",
    "double_digit = [\n",
    "    (name, weight)\n",
    "    for name, weight in feature_coefs_sorted\n",
    "    if weight >= 0.1 or weight <= -0.1\n",
    "]\n",
    "\n",
    "for name, weight in double_digit:\n",
    "    print(name, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQeAHo7mqQ3m"
   },
   "source": [
    "## Interpretable feature analysis:\n",
    "We filtered for the features with the strongest coefficient signals (absolute value >= 0.1).\n",
    "\n",
    "### Positive signals for fraud:\n",
    "- According to this SVM model, if information about the \"number of months in previous registered address of the applicant\" is missing, this is a relatively strong signal for fraud. One potential reason is that fraudsters tend to avoid providing a traceable address history.\n",
    "- According to this SVM model, the device OS being windows is a relatively strong signal for fraud. One possible explanation is that fraudsters often operate from cheap and widely available Windows environments.\n",
    "\n",
    "### Negative signals for fraud:\n",
    "- According to this SVM, the user keeping the session alive on session logout is a negative indicator of fraud. A potential reason for this is that fraudsters tend to avoid being connected to the system longer than they have to be.\n",
    "- According to this SVM model, the similarity in the applicant's name to the email name is a negative indicator for fraud. One possible explanation is that fraudsters often use randomly generated or non-identifying email handles.\n",
    "- According to this SVM model, the validity of the home phone number is a negative indicator of fraud. One possible explanation is that fraudsters often supply fake or burner phone numbers.\n",
    "- According to this SVM model, the applicant having other cards with the same banking company is a negative signal of fraud. This may be because having multiple cards with the same company under the same identity increases the fraudster's detectability, so fraudsters generally avoid this.\n",
    "- Housing status being BE, BB, and BC are negative indicators of fraud according to this model. However, given that BE, BB, and BC are anonymized values, we can't say much more beyond this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_UzlYUUUE7e"
   },
   "source": [
    "# STAGE 2: RBF KERNEL SVM\n",
    "\n",
    "## Methodology:\n",
    "Due to the high memory complexity of storing Gram matrices while training kernel SVMs, we will chunk the training data into manageably-sized chunks, train an SVM model on each of them, then predict 1 if at least k out of N of the SVM models predicts 1.\n",
    "\n",
    "Fix N = 7.\n",
    "\n",
    "We will divide the training dataset into stratified folds of size ~10000. We will then create 3 splits of 8 stratified folds each. For each split, 7 of the folds will be used to train 7 SVM models while the remaining fold will be used as a validation set. This means a total of 24 folds are required. Luckily, we have around 794989/10000 (approx 79) folds available to us. We will do hyperparameter search by finding the hyperparameters that maximize the average F2 score across all 3 splits.\n",
    "\n",
    "Our hyperparameters will be:\n",
    "- C (as in linear kernel SVM)\n",
    "- R (as in linear kernel SVM)\n",
    "- Gamma (for the RBF kernel)\n",
    "- K (the number of learners needed to make a positive prediction)\n",
    "\n",
    "Search ranges:\n",
    "- C: [10^-2,10]\n",
    "- R: [10,1000]\n",
    "- Gamma: [10^-3,1]\n",
    "- K: [1,7]\n",
    "\n",
    "Due to the curse of dimensionality making the hyperparameter search grid combinatorially infeasible to cover via a systematic/exhaustive search, we will use random search instead. Our hyperparameter combination candidates list is created by randomly sampling 10 different (C, R, Gamma) hyperparameter combinations from their corresponding logspaces, then concatenating each of them with each K in 1:7, creating a list of 4-D hyperparameter coordinates (C, R, Gamma, K).\n",
    "\n",
    "After finding our optimal (C, R, Gamma, K) combination, we will then create 7 stratified folds of size ~20000. We will then train 7 RBF-kernel SVM models using the (C, R, Gamma) hyperparameters. Our final model will predict 1 iff at least K out of the 7 models predicts 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jbaev9Ya-LCC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "SVC_hyperparameter_combinations = []\n",
    "for i in range(10):\n",
    "  C_exp = random.uniform(-2,1)\n",
    "  R_exp = random.uniform(1,3)\n",
    "  Gamma_exp = random.uniform(-3,0)\n",
    "  SVC_hyperparameter_combinations.append((10**C_exp,10**R_exp,10**Gamma_exp))\n",
    "\n",
    "Ks = [1,2,3,4,5,6,7]\n",
    "\n",
    "hyperparameter_combinations = [\n",
    "    (C, R, Gamma, k)\n",
    "    for (C, R, Gamma) in SVC_hyperparameter_combinations\n",
    "    for k in Ks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-AHU9o6PSRT",
    "outputId": "6ce936ba-2c3f-481a-991b-e3cb31c29b20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds: 79\n",
      "Size of first fold: 10064\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_splits = 79\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=23)\n",
    "\n",
    "# Collect the \"folds\" = val indices from each split\n",
    "fold_indices = []\n",
    "\n",
    "for _, val_idx in skf.split(X_train_full, y_train_full):\n",
    "    fold_indices.append(val_idx)\n",
    "\n",
    "print(\"Number of folds:\", len(fold_indices))  # should be 79\n",
    "print(\"Size of first fold:\", len(fold_indices[0]))\n",
    "\n",
    "# We only need the first 24 folds\n",
    "folds_for_splits = fold_indices[:24]\n",
    "\n",
    "# Now group them into 3 sublists of 8 folds each\n",
    "splits = [folds_for_splits[i:i + 8] for i in range(0, 24, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25MzJbmdj3a-",
    "outputId": "7ee4d883-4cc7-4d18-a260-3df0dc7d9cf6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVMs trained: 100%|███████████████████████████████████████████████| 210/210 [32:08<00:00,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.40852364882186243, 22.47084293386643, 0.0012593616869968059, 2) 0.261757771381254\n",
      "(0.40852364882186243, 22.47084293386643, 0.0012593616869968059, 3) 0.2547957456786937\n",
      "(0.40852364882186243, 22.47084293386643, 0.0012593616869968059, 4) 0.2547457143399014\n",
      "(0.04795099213571773, 22.391279582061784, 0.005481646677430149, 1) 0.2476590001832328\n",
      "(0.40852364882186243, 22.47084293386643, 0.0012593616869968059, 5) 0.2439292907827676\n",
      "(0.40852364882186243, 22.47084293386643, 0.0012593616869968059, 1) 0.24223896517157362\n",
      "(0.6932901565519473, 649.2000733242538, 0.00615594122894976, 5) 0.23515579071134626\n",
      "(1.5932984587626728, 115.51725592971276, 0.0010363448056037567, 7) 0.23313009244004737\n",
      "(0.6932901565519473, 649.2000733242538, 0.00615594122894976, 4) 0.2314636345887424\n",
      "(0.09160070752777136, 484.83921777028655, 0.022120835730685556, 4) 0.22786097739567143\n",
      "(0.09160070752777136, 484.83921777028655, 0.022120835730685556, 5) 0.22184366357265842\n",
      "(0.40852364882186243, 22.47084293386643, 0.0012593616869968059, 6) 0.21964586303513234\n",
      "(0.6932901565519473, 649.2000733242538, 0.00615594122894976, 3) 0.21608731963787667\n",
      "(1.5932984587626728, 115.51725592971276, 0.0010363448056037567, 6) 0.213534364759566\n",
      "(0.09160070752777136, 484.83921777028655, 0.022120835730685556, 3) 0.20988615523028478\n",
      "(0.04795099213571773, 22.391279582061784, 0.005481646677430149, 2) 0.20795913785361472\n",
      "(8.47422345782953, 376.1817789801277, 0.016345277854939364, 1) 0.2036289237064556\n",
      "(0.09160070752777136, 484.83921777028655, 0.022120835730685556, 2) 0.1986302262463064\n",
      "(0.40852364882186243, 22.47084293386643, 0.0012593616869968059, 7) 0.1981655301680233\n",
      "(1.5932984587626728, 115.51725592971276, 0.0010363448056037567, 5) 0.19734443326895135\n",
      "(0.6932901565519473, 649.2000733242538, 0.00615594122894976, 6) 0.19426836544936466\n",
      "(0.09160070752777136, 484.83921777028655, 0.022120835730685556, 6) 0.18508001664983972\n",
      "(0.38762454297923965, 578.3234360949941, 0.0010148302493256187, 7) 0.18184459907771325\n",
      "(0.6932901565519473, 649.2000733242538, 0.00615594122894976, 2) 0.1763224393995152\n",
      "(1.5932984587626728, 115.51725592971276, 0.0010363448056037567, 4) 0.1751926732023856\n",
      "(0.6932901565519473, 649.2000733242538, 0.00615594122894976, 7) 0.17047249321999447\n",
      "(0.09160070752777136, 484.83921777028655, 0.022120835730685556, 7) 0.16653044585945762\n",
      "(8.47422345782953, 376.1817789801277, 0.016345277854939364, 2) 0.16524833657654148\n",
      "(0.04795099213571773, 22.391279582061784, 0.005481646677430149, 3) 0.15731569594622027\n",
      "(1.5932984587626728, 115.51725592971276, 0.0010363448056037567, 3) 0.15283085426344342\n",
      "(0.09160070752777136, 484.83921777028655, 0.022120835730685556, 1) 0.14970354972549083\n",
      "(0.38762454297923965, 578.3234360949941, 0.0010148302493256187, 6) 0.1444252032549358\n",
      "(0.04795099213571773, 22.391279582061784, 0.005481646677430149, 4) 0.14096959394058803\n",
      "(1.5932984587626728, 115.51725592971276, 0.0010363448056037567, 2) 0.1296070863195584\n",
      "(0.6932901565519473, 649.2000733242538, 0.00615594122894976, 1) 0.12842061321117448\n",
      "(0.38762454297923965, 578.3234360949941, 0.0010148302493256187, 5) 0.12244369389125709\n",
      "(0.04795099213571773, 22.391279582061784, 0.005481646677430149, 5) 0.10926972707548248\n",
      "(0.38762454297923965, 578.3234360949941, 0.0010148302493256187, 4) 0.1038489389661565\n",
      "(1.5932984587626728, 115.51725592971276, 0.0010363448056037567, 1) 0.10310519036473663\n",
      "(0.38762454297923965, 578.3234360949941, 0.0010148302493256187, 3) 0.0869216624186534\n",
      "(8.47422345782953, 376.1817789801277, 0.016345277854939364, 3) 0.08528100527127863\n",
      "(0.04795099213571773, 22.391279582061784, 0.005481646677430149, 6) 0.07790258701790641\n",
      "(0.38762454297923965, 578.3234360949941, 0.0010148302493256187, 2) 0.07312300564530792\n",
      "(0.38762454297923965, 578.3234360949941, 0.0010148302493256187, 1) 0.06100321239946784\n",
      "(8.47422345782953, 376.1817789801277, 0.016345277854939364, 4) 0.04656722320240725\n",
      "(0.04795099213571773, 22.391279582061784, 0.005481646677430149, 7) 0.02371072967458947\n",
      "(8.47422345782953, 376.1817789801277, 0.016345277854939364, 5) 0.019766130357489656\n",
      "(1.2785599618357162, 123.46059207796709, 0.12614303532709845, 1) 0.015294462247058183\n",
      "(8.47422345782953, 376.1817789801277, 0.016345277854939364, 6) 0.007984593082186802\n",
      "(0.0693177484008567, 722.9896699022486, 0.26154976391994783, 1) 0.007946520579499303\n",
      "(0.0693177484008567, 722.9896699022486, 0.26154976391994783, 2) 0.004035512510088781\n",
      "(1.2785599618357162, 123.46059207796709, 0.12614303532709845, 2) 0.004025764895330112\n",
      "(0.8203750196012581, 93.59037523145923, 0.26240363858599347, 1) 0.004016064257028113\n",
      "(8.47422345782953, 376.1817789801277, 0.016345277854939364, 7) 0.003977724741447892\n",
      "(0.8203750196012581, 93.59037523145923, 0.26240363858599347, 2) 0.0\n",
      "(0.8203750196012581, 93.59037523145923, 0.26240363858599347, 3) 0.0\n",
      "(0.8203750196012581, 93.59037523145923, 0.26240363858599347, 4) 0.0\n",
      "(0.8203750196012581, 93.59037523145923, 0.26240363858599347, 5) 0.0\n",
      "(0.8203750196012581, 93.59037523145923, 0.26240363858599347, 6) 0.0\n",
      "(0.8203750196012581, 93.59037523145923, 0.26240363858599347, 7) 0.0\n",
      "(1.2785599618357162, 123.46059207796709, 0.12614303532709845, 3) 0.0\n",
      "(1.2785599618357162, 123.46059207796709, 0.12614303532709845, 4) 0.0\n",
      "(1.2785599618357162, 123.46059207796709, 0.12614303532709845, 5) 0.0\n",
      "(1.2785599618357162, 123.46059207796709, 0.12614303532709845, 6) 0.0\n",
      "(1.2785599618357162, 123.46059207796709, 0.12614303532709845, 7) 0.0\n",
      "(0.0693177484008567, 722.9896699022486, 0.26154976391994783, 3) 0.0\n",
      "(0.0693177484008567, 722.9896699022486, 0.26154976391994783, 4) 0.0\n",
      "(0.0693177484008567, 722.9896699022486, 0.26154976391994783, 5) 0.0\n",
      "(0.0693177484008567, 722.9896699022486, 0.26154976391994783, 6) 0.0\n",
      "(0.0693177484008567, 722.9896699022486, 0.26154976391994783, 7) 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "\n",
    "total_trains = 3 * 10 * 7\n",
    "progress_bar = tqdm(total=total_trains, desc=\"SVMs trained\", ncols=100)\n",
    "\n",
    "running_F2_scores = dict.fromkeys(hyperparameter_combinations, 0) # stores running total of F2 scores across folds\n",
    "\n",
    "for split in splits:\n",
    "  X_training_sets = []\n",
    "  y_training_sets = []\n",
    "  for i in range(7):\n",
    "    X_training_sets.append(X_train_full[split[i]])\n",
    "    y_training_sets.append(y_train_full[split[i]])\n",
    "  X_val = X_train_full[split[7]]\n",
    "  y_val = y_train_full[split[7]]\n",
    "\n",
    "  for SVC_hyper in SVC_hyperparameter_combinations:\n",
    "      C = SVC_hyper[0]\n",
    "      R = SVC_hyper[1]\n",
    "      Gamma = SVC_hyper[2]\n",
    "\n",
    "      model_list = []\n",
    "      for i in range(7):\n",
    "        model = SVC(\n",
    "              kernel=\"rbf\",\n",
    "              C=C,\n",
    "              gamma=Gamma,\n",
    "              class_weight={0: 1, 1: R},\n",
    "              probability=False\n",
    "          )\n",
    "        model.fit(X_training_sets[i], y_training_sets[i])\n",
    "        model_list.append(model)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "      pred_list = []\n",
    "      for model in model_list:\n",
    "        preds = model.predict(X_val)\n",
    "        pred_list.append(preds)\n",
    "      pred_list = np.array(pred_list)\n",
    "\n",
    "      total_preds = np.sum(pred_list, axis=0)\n",
    "\n",
    "      for k in range(1,8):\n",
    "        f2 = fbeta_score(y_val, (total_preds >= k).astype(int), beta=2, zero_division=0)\n",
    "        running_F2_scores[(C,R,Gamma,k)] += f2\n",
    "\n",
    "progress_bar.close()\n",
    "average_F2_scores = {hyper: running_F2_scores[hyper] / 3 for hyper in running_F2_scores}\n",
    "\n",
    "# Sort the hyperparameters by F2 score:\n",
    "for hyper, score in sorted(average_F2_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(hyper, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eHct-2_-72nO",
    "outputId": "c8a8a9e8-c63f-4baa-fd7a-8c85eb240acd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds: 79\n",
      "Chunk 1: 20128 samples\n",
      "Chunk 2: 20128 samples\n",
      "Chunk 3: 20128 samples\n",
      "Chunk 4: 20128 samples\n",
      "Chunk 5: 20128 samples\n",
      "Chunk 6: 20128 samples\n",
      "Chunk 7: 20126 samples\n"
     ]
    }
   ],
   "source": [
    "# We set (C, R, Gamma, k) = (0.40852364882186243, 22.47084293386643, 0.0012593616869968059, 2)\n",
    "# for our final model, we will train each of the 7 SVM models on ~20000 points instead of ~10000\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "n_splits = 79\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=23)\n",
    "\n",
    "# Collect the \"folds\" = validation indices from each split\n",
    "fold_indices = []\n",
    "for _, val_idx in skf.split(X_train_full, y_train_full):\n",
    "    fold_indices.append(val_idx)\n",
    "\n",
    "print(\"Number of folds:\", len(fold_indices))  # should be 79\n",
    "\n",
    "# Create the 7 final index arrays (each built from 2 folds)\n",
    "# We use the first 14 folds -> 7 groups × 2 folds per group\n",
    "final_train_chunks = []\n",
    "\n",
    "for i in range(7):\n",
    "    fold1 = fold_indices[2*i]\n",
    "    fold2 = fold_indices[2*i + 1]\n",
    "\n",
    "    merged = np.concatenate([fold1, fold2])\n",
    "    final_train_chunks.append(merged)\n",
    "\n",
    "# Inspect sizes:\n",
    "for i, arr in enumerate(final_train_chunks):\n",
    "    print(f\"Chunk {i+1}: {len(arr)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QoebBisSCEdm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "C, R, Gamma = (0.40852364882186243, 22.47084293386643, 0.0012593616869968059)\n",
    "\n",
    "model_list = []\n",
    "for i in range(7):\n",
    "  model = SVC(\n",
    "        kernel=\"rbf\",\n",
    "        C=C,\n",
    "        gamma=Gamma,\n",
    "        class_weight={0: 1, 1: R},\n",
    "        probability=False\n",
    "    )\n",
    "  model.fit(X_train_full[final_train_chunks[i]], y_train_full[final_train_chunks[i]])\n",
    "  model_list.append(model)\n",
    "\n",
    "pred_list = []\n",
    "for model in model_list:\n",
    "  preds = model.predict(X_test_full)\n",
    "  pred_list.append(preds)\n",
    "pred_list = np.array(pred_list)\n",
    "\n",
    "total_preds = np.sum(pred_list, axis=0)\n",
    "final_preds = (total_preds >= 2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aES2JDPNI7aG",
    "outputId": "92d66042-e066-49bf-8bf4-a12ec79ba07a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model F2 Score: 0.31344168354492913\n",
      "Final model Recall: 0.4409312022237665\n",
      "Final model Precision: 0.14534417592486543\n",
      "Final model Accuracy: 0.9557535937096058\n",
      "Final model ROC AUC: 0.7313537579325938\n",
      "Final model PR AUC: 0.10456909034776057\n"
     ]
    }
   ],
   "source": [
    "# F2 Score:\n",
    "f2 = fbeta_score(y_test_full, final_preds, beta=2, zero_division=0)\n",
    "print(\"Final model F2 Score: \" + str(f2))\n",
    "\n",
    "# Recall:\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_test_full, final_preds)\n",
    "print(\"Final model Recall: \" + str(recall))\n",
    "\n",
    "# Precision:\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test_full, final_preds)\n",
    "print(\"Final model Precision: \" + str(precision))\n",
    "\n",
    "# Accuracy:\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test_full, final_preds)\n",
    "print(\"Final model Accuracy: \" + str(accuracy))\n",
    "\n",
    "# ROC AUC and PR AUC\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "roc_auc = roc_auc_score(y_test_full, total_preds)\n",
    "pr_auc = average_precision_score(y_test_full, total_preds)\n",
    "\n",
    "print(\"Final model ROC AUC:\", roc_auc)\n",
    "print(\"Final model PR AUC:\", pr_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMvkIvRI2E6f"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4pNhKxGv0N7"
   },
   "source": [
    "Here are the results for the RBF ensemble method:\n",
    "```\n",
    "Final model F2 Score: 0.31284385315594676\n",
    "Final model Recall: 0.4346768589298124\n",
    "Final model Precision: 0.14748879981136526\n",
    "Final model Accuracy: 0.956792562350313\n",
    "```\n",
    "\n",
    "Here are the results for the linear kernel method:\n",
    "```\n",
    "Final model F2 Score: 0.31082471435533654\n",
    "Final model Recall: 0.3988881167477415\n",
    "Final model Precision: 0.1650611071171819\n",
    "Final model Accuracy: 0.963236119037515\n",
    "```\n",
    "\n",
    "The RBF ensemble method and linear kernel method achieved similar F2 scores, though the RBF ensemble had a significantly better recall score and a slightly worse precision.\n",
    "\n",
    "Note that the RBF ensemble method used here is pretty much guaranteed to be suboptimal, since N = 7 is a very small ensemble size and I only did a coarse logspace search of hyperparameters and didn't do a finer linear space search afterwards. Therefore, the F2 score of the RBF ensemble method will likely see some non-neglibile improvements with further tuning.\n",
    "\n",
    "However, a fundamental limitation of using nonlinear kernels here is that complexity explosion (time and space) prevent the model from being feasibly trained on *all* the training data. Moreover, there is no compelling reason to believe that using a polynomial kernel will achieve significantly better results, since it shares much of the same computational limitations as RBF kernel. So although there is likely some more \"juice\" to be squeezed out of the nonlinear kernel SVM approach, our efforts will likely be better rewarded by focusing on XGBoost from now on instead. We will keep the test results of the RBF ensemble method as a baseline benchmark."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
